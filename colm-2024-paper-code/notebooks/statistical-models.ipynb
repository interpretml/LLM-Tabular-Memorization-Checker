{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical predictors on the different dataset versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import tabmemcheck\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "from statutils import loo_eval, accuracy, roc_auc\n",
    "import yaml\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load list of datasets from yaml file\n",
    "with open('datasets.yaml') as file:\n",
    "    datasets = yaml.load(file, Loader=yaml.FullLoader)['datasets']\n",
    "\n",
    "versions = ['original', 'perturbed', 'task', 'statistical']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['datasets/tabular/spaceship-titanic-train.csv',\n",
       "  'config/transform/spaceship-titanic.yaml'],\n",
       " ['datasets/tabular/acs-income-2022.csv', 'config/transform/acs-income.yaml'],\n",
       " ['datasets/tabular/acs-travel-2022.csv', 'config/transform/acs-travel.yaml'],\n",
       " ['datasets/tabular/icu.csv', 'config/transform/icu.yaml'],\n",
       " ['datasets/tabular/heloc_dataset_v1.csv', 'config/transform/fico.yaml']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = datasets[5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create comparable numerical representations of the different dataset versions\n",
    "#### In particular, we need to create dummy variables consistently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/tabular/spaceship-titanic-train.csv drop: ['PassengerId', 'Cabin', 'Name']\n",
      "datasets/tabular/spaceship-titanic-train.csv dummies: Index(['HomePlanet', 'CryoSleep', 'Destination', 'VIP'], dtype='object')\n",
      "(8693, 12)\n",
      "(8693, 12)\n",
      "(8693, 13)\n",
      "(8693, 16)\n",
      "datasets/tabular/acs-income-2022.csv drop: []\n",
      "datasets/tabular/acs-income-2022.csv dummies: Index(['Class of worker', 'Educational attainment', 'Marital status',\n",
      "       'Occupation', 'Place of birth', 'Sex', 'Recoded race'],\n",
      "      dtype='object')\n",
      "(200577, 792)\n",
      "(200577, 792)\n",
      "(200577, 791)\n",
      "(200577, 791)\n",
      "datasets/tabular/acs-travel-2022.csv drop: []\n",
      "datasets/tabular/acs-travel-2022.csv dummies: Index(['Educational attainment', 'Marital status', 'Sex', 'Disability',\n",
      "       'Employment status of parents', 'Lived here 1 year ago',\n",
      "       'Recorded race', 'Living Area', 'State', 'Citizenship', 'Occupation',\n",
      "       'Place of Work Area'],\n",
      "      dtype='object')\n",
      "(177724, 1023)\n",
      "(177724, 1023)\n",
      "(177724, 1023)\n",
      "(177724, 1024)\n",
      "datasets/tabular/icu.csv drop: []\n",
      "datasets/tabular/icu.csv dummies: Index([], dtype='object')\n",
      "(102, 10)\n",
      "(102, 10)\n",
      "(102, 10)\n",
      "(102, 10)\n",
      "datasets/tabular/heloc_dataset_v1.csv drop: []\n",
      "datasets/tabular/heloc_dataset_v1.csv dummies: Index([], dtype='object')\n",
      "(10459, 23)\n",
      "(10459, 23)\n",
      "(10459, 23)\n",
      "(10459, 23)\n"
     ]
    }
   ],
   "source": [
    "datasets_numeric = {}\n",
    "\n",
    "for csv_file, yaml_file in datasets:\n",
    "    datasets_numeric[csv_file] = {}\n",
    "    df = tabmemcheck.datasets.load_dataset(csv_file, yaml_file, 'original', seed=2) # 0\n",
    "    df = df.drop(df.columns[-1], axis=1) \n",
    "    # on some datasets, we have to drop certain categorial columns because it would result in too many dummy variables\n",
    "    drop_cols = []\n",
    "    if 'spaceship' in csv_file:\n",
    "        drop_cols = ['PassengerId', 'Cabin', 'Name']\n",
    "    elif 'titanic' in csv_file:\n",
    "        drop_cols = ['Name', 'Ticket', 'Cabin']\n",
    "    print(csv_file, 'drop:', drop_cols)\n",
    "    drop_cols = [df.columns.get_loc(col) for col in drop_cols] # the indices of the columns\n",
    "    df = df.drop(df.columns[drop_cols], axis=1)\n",
    "    # use the original version to deterime the features that should be transformed to dummy variables - in all versions (!)\n",
    "    dummy_cols = df.select_dtypes(include=['object', 'string', 'category']).columns\n",
    "    print(csv_file, 'dummies:', dummy_cols) \n",
    "    dummy_cols = [df.columns.get_loc(col) for col in dummy_cols] # the indices of the columns\n",
    "\n",
    "    for version in versions:\n",
    "        df = tabmemcheck.datasets.load_dataset(csv_file, yaml_file, version)\n",
    "\n",
    "        # the last column is the target, extract it\n",
    "        y = df.iloc[:, -1]\n",
    "        df = df.drop(df.columns[-1], axis=1)\n",
    "\n",
    "        # if the target is not numeric, convert it to categorical\n",
    "        if y.dtype in ['object', 'string', 'category']:\n",
    "            y = y.astype('category').cat.codes\n",
    "\n",
    "        # drop the columns that should not be used\n",
    "        df = df.drop(df.columns[drop_cols], axis=1)\n",
    "\n",
    "        # create dummy variables\n",
    "        df = pd.get_dummies(df, columns=df.columns[dummy_cols], drop_first=True)\n",
    "        print(df.values.shape)\n",
    "        \n",
    "        # Ensure all data is numeric now\n",
    "        for col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        \n",
    "        # Fill NaN values with 0\n",
    "        df = df.fillna(0)\n",
    "\n",
    "        # store the numeric dataset\n",
    "        datasets_numeric[csv_file][version] = (df.values, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statutils import fit_logistic_regression_cv\n",
    "\n",
    "# fit-predict function\n",
    "def fit_predict(X_train, y_train, X_test):\n",
    "    clf = fit_logistic_regression_cv(X_train, y_train, random_state=2) # 123\n",
    "    return clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/tabular/acs-travel-2022.csv original\n",
      "Accuracy: 0.64, 95%-Confidence Interval: (0.63, 0.65)\n",
      "datasets/tabular/acs-travel-2022.csv perturbed\n",
      "Accuracy: 0.64, 95%-Confidence Interval: (0.62, 0.65)\n",
      "datasets/tabular/acs-travel-2022.csv task\n",
      "Accuracy: 0.64, 95%-Confidence Interval: (0.63, 0.65)\n",
      "datasets/tabular/acs-travel-2022.csv statistical\n",
      "Accuracy: 0.64, 95%-Confidence Interval: (0.63, 0.65)\n",
      "datasets/tabular/icu.csv original\n",
      "Accuracy: 0.76, 95%-Confidence Interval: (0.68, 0.84)\n",
      "datasets/tabular/icu.csv perturbed\n",
      "Accuracy: 0.76, 95%-Confidence Interval: (0.68, 0.84)\n",
      "datasets/tabular/icu.csv task\n",
      "Accuracy: 0.77, 95%-Confidence Interval: (0.69, 0.84)\n",
      "datasets/tabular/icu.csv statistical\n",
      "Accuracy: 0.75, 95%-Confidence Interval: (0.66, 0.82)\n",
      "datasets/tabular/heloc_dataset_v1.csv original\n",
      "Accuracy: 0.70, 95%-Confidence Interval: (0.67, 0.72)\n",
      "datasets/tabular/heloc_dataset_v1.csv perturbed\n",
      "Accuracy: 0.70, 95%-Confidence Interval: (0.68, 0.72)\n",
      "datasets/tabular/heloc_dataset_v1.csv task\n",
      "Accuracy: 0.69, 95%-Confidence Interval: (0.67, 0.71)\n",
      "datasets/tabular/heloc_dataset_v1.csv statistical\n",
      "Accuracy: 0.70, 95%-Confidence Interval: (0.68, 0.72)\n"
     ]
    }
   ],
   "source": [
    "for csv_file, yaml_file in datasets:\n",
    "    for version in versions:\n",
    "        # the numeric dataset\n",
    "        X_data, y_data = datasets_numeric[csv_file][version]\n",
    "\n",
    "        # for small datasets, perform leave-one-out evaluation\n",
    "        if X_data.shape[0] > 2500:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X_data, y_data, test_size=0.2, random_state=42\n",
    "            )\n",
    "            # on large datasets, reduce the number of training points for logistic regression\n",
    "            X_train, y_train = X_train[:15000], y_train[:15000]\n",
    "            X_test, y_test = X_test[:5000], y_test[:5000]\n",
    "            y_pred = fit_predict(X_train, y_train, X_test)\n",
    "        else:\n",
    "            # leave-one-out evaluation\n",
    "            y_pred = loo_eval(X_data, y_data, fit_predict)\n",
    "            y_test = y_data\n",
    "\n",
    "        # evaluate\n",
    "        print(f'{csv_file} {version}')\n",
    "        accuracy(y_test, y_pred)\n",
    "        #roc_auc(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trained on original, tested on perturbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/csv/tabular/iris.csv statistical\n",
      "Accuracy: 0.97, 95%-Confidence Interval: (0.93, 0.99)\n",
      "Accuracy: 0.95, 95%-Confidence Interval: (0.91, 0.98)\n",
      "datasets/csv/tabular/adult-train.csv statistical\n",
      "Accuracy: 0.86, 95%-Confidence Interval: (0.85, 0.87)\n",
      "Accuracy: 0.86, 95%-Confidence Interval: (0.85, 0.87)\n",
      "datasets/csv/tabular/openml-diabetes.csv statistical\n",
      "Accuracy: 0.78, 95%-Confidence Interval: (0.75, 0.81)\n",
      "Accuracy: 0.77, 95%-Confidence Interval: (0.74, 0.80)\n",
      "datasets/csv/tabular/uci-wine.csv statistical\n",
      "Accuracy: 0.98, 95%-Confidence Interval: (0.96, 0.99)\n",
      "Accuracy: 0.98, 95%-Confidence Interval: (0.96, 0.99)\n",
      "datasets/csv/tabular/titanic-train.csv statistical\n",
      "Accuracy: 0.79, 95%-Confidence Interval: (0.76, 0.81)\n",
      "Accuracy: 0.79, 95%-Confidence Interval: (0.76, 0.81)\n",
      "datasets/csv/tabular/spaceship-titanic-train.csv statistical\n",
      "Accuracy: 0.78, 95%-Confidence Interval: (0.76, 0.80)\n",
      "Accuracy: 0.78, 95%-Confidence Interval: (0.76, 0.80)\n",
      "datasets/csv/tabular/acs-income-2022.csv statistical\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for csv_file, yaml_file in datasets:\n",
    "    # the numeric dataset\n",
    "    X_data, y_data = datasets_numeric[csv_file]['original']\n",
    "    X_data_p, y_data_p = datasets_numeric[csv_file]['perturbed']\n",
    "\n",
    "    # for small datasets, perform leave-one-out evaluation\n",
    "    if X_data.shape[0] > 2500:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_data, y_data, test_size=0.2, random_state=42\n",
    "        )\n",
    "        _, X_test_p, _, _ = train_test_split(\n",
    "            X_data_p, y_data_p, test_size=0.2, random_state=42\n",
    "        )\n",
    "        # on large datasets, reduce the number of training points for logistic regression\n",
    "        X_train, y_train = X_train[:15000], y_train[:15000]\n",
    "        X_test, y_test = X_test[:5000], y_test[:5000]\n",
    "        X_test_p = X_test_p[:5000]\n",
    "        y_pred = fit_predict(X_train, y_train, X_test)\n",
    "        y_pred_p = fit_predict(X_train, y_train, X_test_p)\n",
    "    else:\n",
    "        # leave-one-out evaluation\n",
    "        y_pred = loo_eval(X_data, y_data, fit_predict)\n",
    "        y_pred_p = loo_eval(X_data, y_data, fit_predict, X_test=X_data_p)\n",
    "        y_test = y_data\n",
    "\n",
    "    # evaluate\n",
    "    print(f'{csv_file} {version}')\n",
    "    accuracy(y_test, y_pred)\n",
    "    accuracy(y_test, y_pred_p)\n",
    "    #roc_auc(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit gradient boosted tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statutils import fit_gbtree_cv\n",
    "\n",
    "# fit-predict function\n",
    "def fit_predict(X_train, y_train, X_test):\n",
    "    clf = fit_gbtree_cv(X_train, y_train)\n",
    "    return clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/tabular/spaceship-titanic-train.csv original\n",
      "Accuracy: 0.78, 95%-Confidence Interval: (0.76, 0.80), Standard error: 0.01\n",
      "datasets/tabular/spaceship-titanic-train.csv perturbed\n",
      "Accuracy: 0.77, 95%-Confidence Interval: (0.75, 0.79), Standard error: 0.01\n",
      "datasets/tabular/spaceship-titanic-train.csv task\n",
      "Accuracy: 0.77, 95%-Confidence Interval: (0.75, 0.79), Standard error: 0.01\n",
      "datasets/tabular/spaceship-titanic-train.csv statistical\n",
      "Accuracy: 0.78, 95%-Confidence Interval: (0.76, 0.80), Standard error: 0.01\n",
      "datasets/tabular/acs-income-2022.csv original\n",
      "Accuracy: 0.80, 95%-Confidence Interval: (0.79, 0.81), Standard error: 0.01\n",
      "datasets/tabular/acs-income-2022.csv perturbed\n",
      "Accuracy: 0.80, 95%-Confidence Interval: (0.79, 0.81), Standard error: 0.01\n",
      "datasets/tabular/acs-income-2022.csv task\n",
      "Accuracy: 0.80, 95%-Confidence Interval: (0.79, 0.81), Standard error: 0.01\n",
      "datasets/tabular/acs-income-2022.csv statistical\n",
      "Accuracy: 0.80, 95%-Confidence Interval: (0.79, 0.81), Standard error: 0.01\n",
      "datasets/tabular/acs-travel-2022.csv original\n",
      "Accuracy: 0.67, 95%-Confidence Interval: (0.66, 0.69), Standard error: 0.01\n",
      "datasets/tabular/acs-travel-2022.csv perturbed\n",
      "Accuracy: 0.68, 95%-Confidence Interval: (0.67, 0.69), Standard error: 0.01\n",
      "datasets/tabular/acs-travel-2022.csv task\n",
      "Accuracy: 0.67, 95%-Confidence Interval: (0.66, 0.68), Standard error: 0.01\n",
      "datasets/tabular/acs-travel-2022.csv statistical\n",
      "Accuracy: 0.67, 95%-Confidence Interval: (0.66, 0.68), Standard error: 0.01\n",
      "datasets/tabular/icu.csv original\n",
      "Accuracy: 0.67, 95%-Confidence Interval: (0.57, 0.75), Standard error: 0.05\n",
      "datasets/tabular/icu.csv perturbed\n",
      "Accuracy: 0.63, 95%-Confidence Interval: (0.53, 0.72), Standard error: 0.05\n",
      "datasets/tabular/icu.csv task\n",
      "Accuracy: 0.66, 95%-Confidence Interval: (0.57, 0.75), Standard error: 0.05\n",
      "datasets/tabular/icu.csv statistical\n",
      "Accuracy: 0.66, 95%-Confidence Interval: (0.56, 0.75), Standard error: 0.05\n",
      "datasets/tabular/heloc_dataset_v1.csv original\n",
      "Accuracy: 0.68, 95%-Confidence Interval: (0.66, 0.70), Standard error: 0.01\n",
      "datasets/tabular/heloc_dataset_v1.csv perturbed\n",
      "Accuracy: 0.69, 95%-Confidence Interval: (0.67, 0.71), Standard error: 0.01\n",
      "datasets/tabular/heloc_dataset_v1.csv task\n",
      "Accuracy: 0.69, 95%-Confidence Interval: (0.67, 0.71), Standard error: 0.01\n",
      "datasets/tabular/heloc_dataset_v1.csv statistical\n",
      "Accuracy: 0.68, 95%-Confidence Interval: (0.66, 0.70), Standard error: 0.01\n"
     ]
    }
   ],
   "source": [
    "for csv_file, yaml_file in datasets:\n",
    "    for version in versions:\n",
    "        # the numeric dataset\n",
    "        X_data, y_data = datasets_numeric[csv_file][version]\n",
    "\n",
    "        # for small datasets, perform leave-one-out evaluation\n",
    "        if X_data.shape[0] > 2500:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X_data, y_data, test_size=0.2, random_state=42\n",
    "            )\n",
    "            X_test, y_test = X_test[:5000], y_test[:5000]\n",
    "            # on large datasets, train gradient boosting trees with default parameters\n",
    "            from xgboost import XGBClassifier\n",
    "            clf = XGBClassifier()\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "        else:\n",
    "            # leave-one-out evaluation\n",
    "            y_pred = loo_eval(X_data, y_data, fit_predict)\n",
    "            y_test = y_data\n",
    "\n",
    "        # evaluate\n",
    "        print(f'{csv_file} {version}')\n",
    "        accuracy(y_test, y_pred)\n",
    "        #roc_auc(y, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mkl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
